#[derive(PartialEq, Debug, Copy, Clone)]
pub enum IntegerBase {
    Bin,
    Oct,
    Dec,
    Hex,
}

#[derive(PartialEq, Debug, Copy, Clone)]
pub struct IntegerLiteral<'src> {
    base: IntegerBase,
    value: &'src str,
    suffix: &'src str,
}

#[derive(PartialEq, Debug, Copy, Clone)]
pub enum Token<'source> {
    // Single-character tokens
    LeftParen,
    RightParen,
    LeftBrace,
    RightBrace,
    LeftBracket,
    RightBracket,
    Comma,
    Minus,
    Plus,
    Colon,
    Semicolon,
    Slash,
    Backslash,
    Star,
    Ampersand,
    Pipe,
    // One or two character tokens
    Not,
    NotEqual,
    Equal,
    EqualEqual,
    Greater,
    GreaterEqual,
    Less,
    LessEqual,
    Dot,
    DotDot,
    DotDotEqual,
    // Literals
    Identifier(&'source str),
    String(&'source str),
    Integer(IntegerLiteral<'source>),
    Float(&'source str),
    // Keywords
    And,
    Break,
    Const,
    Continue,
    Else,
    Enum,
    False,
    Fn,
    For,
    If,
    In,
    Let,
    Match,
    Mut,
    Nil,
    Or,
    Pub,
    Return,
    SelfValue,
    SelfType,
    Struct,
    Super,
    This,
    True,
    While,
    // Others
    Eof,
    Err,
}

#[derive(Eq, PartialEq, Debug)]
pub struct SyntaxError {
    pub what: String,
    pub line: u64,
    pub column: u64,
}

pub enum Error {
    NoDigitLiteral,
    InvalidDigitLiteral,
    EmptyExponentFloat,
    FloatLiteralUnsupportedBase,
}

// TODO (1) tokenize using a manual loop and return a token list
// let tokens = tokenize("let answer = 42;");
// Pros: Interface is simple, implementation is verbose but simple
// Cons: Vec<> usage is forced
// Based on https://brunocalza.me/writing-a-simple-lexer-in-rust/
pub mod manual_loop {
    use super::{SyntaxError, Token};

    pub fn tokenize(source: &str) -> Result<Vec<Token>, SyntaxError> {
        let mut tokens: Vec<Token> = Vec::new();
        let mut iter = source.char_indices().peekable();
        let mut line: u64 = 0;
        let mut column: u64 = 0;

        while let Some((index, ch)) = iter.next() {
            column += 1;
            match ch {
                ch if ch.is_whitespace() => match ch {
                    '\n' => {
                        line += 1;
                        column = 1;
                    }
                    _ => {}
                },
                // Single-character tokens
                '(' => tokens.push(Token::LeftParen),
                ')' => tokens.push(Token::RightParen),
                '{' => tokens.push(Token::LeftBrace),
                '}' => tokens.push(Token::RightBrace),
                '[' => tokens.push(Token::LeftBracket),
                ']' => tokens.push(Token::RightBracket),
                ',' => tokens.push(Token::Comma),
                '-' => tokens.push(Token::Minus),
                '+' => tokens.push(Token::Plus),
                ':' => tokens.push(Token::Colon),
                ';' => tokens.push(Token::Semicolon),
                '/' => tokens.push(Token::Slash),
                '\\' => tokens.push(Token::Backslash),
                '*' => tokens.push(Token::Star),
                '&' => tokens.push(Token::Ampersand),
                '|' => tokens.push(Token::Pipe),
                // One or two character tokens
                '!' => match iter.peek() {
                    Some((_, '=')) => {
                        iter.next();
                        tokens.push(Token::NotEqual);
                    }
                    _ => tokens.push(Token::Not),
                },
                '=' => match iter.peek() {
                    Some((_, '=')) => {
                        iter.next();
                        tokens.push(Token::EqualEqual);
                    }
                    _ => tokens.push(Token::Equal),
                },
                '>' => match iter.peek() {
                    Some((_, '=')) => {
                        iter.next();
                        tokens.push(Token::GreaterEqual);
                    }
                    _ => tokens.push(Token::Greater),
                },
                '<' => match iter.peek() {
                    Some((_, '=')) => {
                        iter.next();
                        tokens.push(Token::LessEqual);
                    }
                    _ => tokens.push(Token::Less),
                },
                '.' => match iter.peek() {
                    Some((_, '.')) => {
                        iter.next();
                        match iter.peek() {
                            Some((_, '=')) => {
                                iter.next();
                                tokens.push(Token::DotDotEqual);
                            }
                            _ => tokens.push(Token::DotDot),
                        }
                    }
                    _ => tokens.push(Token::Dot),
                },
                // Literals
                '1'..='9' => {
                    let start_index = index;
                    // Extract number literals
                    // _ is accepted as digit separator
                    // integers:
                    //   can be prefixed by a base (0x, 0o or 0b)
                    //   can be suffixed by a type ({u,i}{8,16,32,64})
                    // floats:
                    //   {integer part}.{decimal part}
                    //   e-notation: 1e6, 7.6e-4
                    //   can be suffixed by a type (f{32,64})
                    match iter
                        .by_ref()
                        .take_while(|&(_index, ch)| /*TODO add all cases*/ ch.is_ascii_digit())
                        .last()
                    {
                        Some((index, _ch)) => {
                            // The iterator is only taking valid chars for numeric literals to the conversion will not fail
                            let n: i64 = source[start_index..=index].parse().unwrap();
                            // tokens.push(Token::IntegerDec(n));
                        }
                        _ => {
                            return Err(SyntaxError {
                                what: String::from("Invalid numeric literal"),
                                line,
                                column,
                            })
                        }
                    }
                }
                ch if ch.is_alphabetic() || ch == '_' => {
                    // TODO add identifiers handling
                }
                _ => {
                    return Err(SyntaxError {
                        what: String::from("Syntax error"),
                        line,
                        column,
                    })
                }
            }
        }
        tokens.push(Token::Eof);
        Ok(tokens)
    }

    #[cfg(test)]
    mod tests {
        use super::*;
        #[test]
        fn tokenizer() {
            let result = tokenize("Hello, world! This (is) a test != working");
            assert_eq!(
                result,
                Ok(vec![
                    Token::Comma,
                    Token::Not,
                    Token::LeftParen,
                    Token::RightParen,
                    Token::NotEqual,
                    Token::Eof
                ])
            );
        }
    }
}

// TODO (1.1) tokenize using a manual loop and return a token list
// let tokens = tokenize("let answer = 42;");
// Pros: Interface is simple, implementation is verbose but simple
// Cons: Vec<> usage is forced
// Based on https://www.michaelfbryan.com/static-analyser-in-rust/book/lex.html
mod manual_loop2 {
    use super::{IntegerBase, IntegerLiteral, SyntaxError, Token};

    fn tokenize_identifier(data: &str) -> Result<(Token, usize), SyntaxError> {
        // we assume data starts with an identifier-compatible character
        if let Some((index, _ch)) = data
            .char_indices()
            .take_while(|&(_index, ch)| is_identifier_char(ch))
            .last()
        {
            return Ok((Token::Identifier(&data[0..=index]), index + 1));
        }
        return Err(SyntaxError {
            what: String::from("not found"),
            line: 0,
            column: 0,
        });
    }

    fn is_identifier_char(ch: char) -> bool {
        return ch.is_alphanumeric() || ch == '_';
    }

    fn is_ascii_digit(ch: char) -> bool {
        return ch.is_ascii_digit() || ch == '_';
    }

    fn is_ascii_bindigit(ch: char) -> bool {
        return ('0'..='1').contains(&ch) || ch == '_';
    }

    fn is_ascii_octdigit(ch: char) -> bool {
        return ('0'..='7').contains(&ch) || ch == '_';
    }

    fn is_ascii_hexdigit(ch: char) -> bool {
        return ch.is_ascii_hexdigit() || ch == '_';
    }

    fn is_ascii_suffix(ch: char) -> bool {
        return ch.is_ascii_alphanumeric() || ch == '_';
    }

    fn tokenize_number_hexdigit(data: &str) -> (&str, usize) {
        match data
            .char_indices()
            .take_while(|&(_index, ch)| is_ascii_hexdigit(ch))
            .last()
        {
            Some((index, _ch)) => (&data[..=index], index + 1),
            None => ("", 0),
        }
    }

    fn tokenize_number(data: &str) -> Result<(Token, usize), SyntaxError> {
        // we assume data starts with a digit

        // Extract base if any
        let base = match &data[0..=1] {
            "0b" => Some(IntegerBase::Bin),
            "0o" => Some(IntegerBase::Oct),
            "0d" => Some(IntegerBase::Dec),
            "0x" => Some(IntegerBase::Hex),
            _ => None,
        };

        // Extract value following base
        if let Some(base) = base {
            let (value, index) = tokenize_number_hexdigit(&data[2..]);
            return Ok((
                Token::Integer(IntegerLiteral {
                    base,
                    value,
                    suffix: "", // TODO handle properly
                }),
                index + 2, // Add two chars for the base
            ));
        }

        // Extract number part until non numeric digit
        let part = data
            .char_indices()
            .take_while(|&(_index, ch)| is_ascii_digit(ch))
            .last();

        match part {
            None => {
                let int = IntegerLiteral {
                    base: IntegerBase::Dec,
                    value: "",  // Obviously empty
                    suffix: "", // TODO handle properly
                };
                return Ok((Token::Integer(int), 0));
            }
            Some((index, _ch)) => {
                let int = IntegerLiteral {
                    base: IntegerBase::Dec,
                    value: &data[0..=index],
                    suffix: "", // TODO handle properly
                };
                return Ok((Token::Integer(int), index + 1));
            }
        }

        return Err(SyntaxError {
            what: String::from("not found"),
            line: 0,
            column: 0,
        });

        // Handle floats and dec
    }

    #[cfg(test)]
    mod tests2 {
        use super::*;
        #[test]
        fn ident_tokenizer() {
            let result = tokenize_identifier("Hello");
            assert_eq!(result, Ok((Token::Identifier("Hello"), 5)));
        }
        fn empty_integer_literal<'src>(base: IntegerBase) -> IntegerLiteral<'src> {
            IntegerLiteral {
                base,
                value: "",
                suffix: "",
            }
        }
        #[test]
        fn dec_tokenizer() {
            let result = tokenize_number("12345");
            assert_eq!(
                result,
                Ok((
                    Token::Integer(IntegerLiteral {
                        base: IntegerBase::Dec,
                        value: "12345",
                        suffix: "",
                    }),
                    5
                ))
            );
        }
        #[test]
        fn bin_base_empty_tokenizer() {
            let result = tokenize_number("0b 0ther");
            assert_eq!(
                result,
                Ok((Token::Integer(empty_integer_literal(IntegerBase::Bin)), 2))
            );
        }
        #[test]
        fn oct_base_empty_tokenizer() {
            let result = tokenize_number("0o 0ther");
            assert_eq!(
                result,
                Ok((Token::Integer(empty_integer_literal(IntegerBase::Oct)), 2))
            );
        }
        #[test]
        fn dec_base_empty_tokenizer() {
            let result = tokenize_number("0d 0ther");
            assert_eq!(
                result,
                Ok((Token::Integer(empty_integer_literal(IntegerBase::Dec)), 2))
            );
        }
        #[test]
        fn hex_base_empty_tokenizer() {
            let result = tokenize_number("0x 0ther");
            assert_eq!(
                result,
                Ok((Token::Integer(empty_integer_literal(IntegerBase::Hex)), 2))
            );
        }
        #[test]
        fn bin_base_tokenizer() {
            let result = tokenize_number("0b12345other+42");
            assert_eq!(
                result,
                Ok((
                    Token::Integer(IntegerLiteral {
                        base: IntegerBase::Bin,
                        value: "12345",
                        suffix: "",
                    }),
                    7
                ))
            );
        }
        #[test]
        fn oct_base_tokenizer() {
            let result = tokenize_number("0o12345other+42");
            assert_eq!(
                result,
                Ok((
                    Token::Integer(IntegerLiteral {
                        base: IntegerBase::Oct,
                        value: "12345",
                        suffix: "",
                    }),
                    7
                ))
            );
        }
        #[test]
        fn dec_base_tokenizer() {
            let result = tokenize_number("0d12345other+42");
            assert_eq!(
                result,
                Ok((
                    Token::Integer(IntegerLiteral {
                        base: IntegerBase::Dec,
                        value: "12345",
                        suffix: "",
                    }),
                    7
                ))
            );
        }
        #[test]
        fn hex_base_tokenizer() {
            let result = tokenize_number("0x12345other+42");
            assert_eq!(
                result,
                Ok((
                    Token::Integer(IntegerLiteral {
                        base: IntegerBase::Hex,
                        value: "12345",
                        suffix: "",
                    }),
                    7
                ))
            );
        }
    }

    pub fn tokenize(source: &str) -> Result<Vec<Token>, SyntaxError> {
        let mut tokens: Vec<Token> = Vec::new();
        let mut iter = source.char_indices().peekable();
        let mut line: u64 = 0;
        let mut column: u64 = 0;

        while let Some((index, ch)) = iter.next() {
            column += 1;
            match ch {
                ch if ch.is_whitespace() => match ch {
                    '\n' => {
                        line += 1;
                        column = 1;
                    }
                    _ => {}
                },
                // Single-character tokens
                '(' => tokens.push(Token::LeftParen),
                ')' => tokens.push(Token::RightParen),
                '{' => tokens.push(Token::LeftBrace),
                '}' => tokens.push(Token::RightBrace),
                '[' => tokens.push(Token::LeftBracket),
                ']' => tokens.push(Token::RightBracket),
                ',' => tokens.push(Token::Comma),
                '.' => tokens.push(Token::Dot),
                '-' => tokens.push(Token::Minus),
                '+' => tokens.push(Token::Plus),
                ':' => tokens.push(Token::Colon),
                ';' => tokens.push(Token::Semicolon),
                '/' => tokens.push(Token::Slash),
                '\\' => tokens.push(Token::Backslash),
                '*' => tokens.push(Token::Star),
                '&' => tokens.push(Token::Ampersand),
                '|' => tokens.push(Token::Pipe),
                // One or two character tokens
                '!' => match iter.peek() {
                    Some((_, '=')) => {
                        iter.next();
                        tokens.push(Token::NotEqual)
                    }
                    _ => tokens.push(Token::Not),
                },
                '=' => match iter.peek() {
                    Some((_, '=')) => {
                        iter.next();
                        tokens.push(Token::EqualEqual)
                    }
                    _ => tokens.push(Token::Equal),
                },
                '>' => match iter.peek() {
                    Some((_, '=')) => {
                        iter.next();
                        tokens.push(Token::GreaterEqual)
                    }
                    _ => tokens.push(Token::Greater),
                },
                '<' => match iter.peek() {
                    Some((_, '=')) => {
                        iter.next();
                        tokens.push(Token::LessEqual)
                    }
                    _ => tokens.push(Token::Less),
                },
                // Literals
                '1'..='9' => {
                    let start_index = index;
                    // Extract number literals
                    // _ is accepted as digit separator
                    // integers:
                    //   can be prefixed by a base (0x, 0o or 0b)
                    //   can be suffixed by a type ({u,i}{8,16,32,64})
                    // floats:
                    //   {integer part}.{decimal part}
                    //   e-notation: 1e6, 7.6e-4
                    //   can be suffixed by a type (f{32,64})
                    match iter
                        .by_ref()
                        .take_while(|&(_index, ch)| /*TODO add all cases*/ ch.is_ascii_digit())
                        .last()
                    {
                        Some((index, _ch)) => {
                            // The iterator is only taking valid chars for numeric literals to the conversion will not fail
                            let n: i64 = source[start_index..=index].parse().unwrap();
                            // tokens.push(Token::Integer(n));
                        }
                        _ => {
                            return Err(SyntaxError {
                                what: String::from("Invalid numeric literal"),
                                line,
                                column,
                            })
                        }
                    }
                }
                ch if ch.is_alphabetic() || ch == '_' => {
                    // TODO add identifiers handling
                }
                _ => {
                    return Err(SyntaxError {
                        what: String::from("Syntax error"),
                        line,
                        column,
                    })
                }
            }
        }
        tokens.push(Token::Eof);
        Ok(tokens)
    }

    #[cfg(test)]
    mod tests {
        use super::*;
        #[test]
        fn tokenizer() {
            let result = tokenize("Hello, world! This (is) a test != working");
            assert_eq!(
                result,
                Ok(vec![
                    Token::Comma,
                    Token::Not,
                    Token::LeftParen,
                    Token::RightParen,
                    Token::NotEqual,
                    Token::Eof
                ])
            );
        }
    }
}

// TODO (2) implement an iterator adapter for &str (and String?)
// let tokens = "let answer = 42;".tokenize().collect::Vec<_>();
// Pros: Only an iterator on the input is returned, the user can collect the
//       tokens however he wants, implementation is more compact than (1)
// Cons: Interface is a bit more complex than (1), but still easy to use
mod iterator {
    use super::SyntaxError;
    use super::Token;

    pub fn tokenize<'a>(source: &'a str) -> impl Iterator<Item = Token> + use<'a> {
        source
            .char_indices()
            .map(|(_index, _char)| Token::Ampersand)
    }
}

// TODO (3) implement a lexer based on https://crates.io/crates/logos for performance comparison
// See https://alic.dev/blog/fast-lexing

// TODO (1.2) implement a Cursor class to ease movements in the input stream
use std::str::Chars;

const EOF_CHAR: char = '\0';

struct Cursor<'src> {
    chars: Chars<'src>,
}

impl<'src> Cursor<'src> {
    pub fn new(input: &'src str) -> Cursor<'src> {
        Cursor {
            chars: input.chars(),
        }
    }

    pub fn as_str(&self) -> &'src str {
        self.chars.as_str()
    }

    pub fn peek(&self) -> char {
        self.chars.clone().next().unwrap_or(EOF_CHAR)
    }

    pub fn peek_nth(&self, n: usize) -> char {
        self.chars.clone().nth(n).unwrap_or(EOF_CHAR)
    }

    pub fn next(&mut self) -> Option<char> {
        self.chars.next()
    }

    pub fn eat_while(&mut self, mut predicate: impl FnMut(char) -> bool) {
        while predicate(self.peek()) && !self.is_eof() {
            self.next();
        }
    }

    pub fn is_eof(&self) -> bool {
        self.chars.as_str().is_empty()
    }
}
